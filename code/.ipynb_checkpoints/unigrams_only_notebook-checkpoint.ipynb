{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.layers import *\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.eager import context\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing in Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(original_file: str, input_path: str):\n",
    "    \"\"\"\n",
    "    :param original_file; Full path to the original dataset\n",
    "    :param input_path; Path to the directory to store processed files, with the filename, but no extension\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    labels = []\n",
    "    with open(original_file, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        for line in file:\n",
    "            line_chars, line_labels = get_chars_and_labels(line)\n",
    "            lines.append(line_chars)\n",
    "            labels.append(line_labels)\n",
    "\n",
    "    with open(input_path+\"_input.utf8\", \"w\", encoding=\"utf-8-sig\") as file:\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                file.write(line+\"\\n\")\n",
    "\n",
    "    with open(input_path+\"_labels.utf8\", \"w\", encoding=\"utf-8-sig\") as file:\n",
    "        for label in labels:\n",
    "            if label.strip():\n",
    "                file.write(label+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def get_chars_and_labels(line: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    :param line; A line from the dataset as str\n",
    "    :return chars; Compressed string for the line\n",
    "    :return labels; Word_segment code for the line. Same len as chars\n",
    "    \"\"\"\n",
    "    chars = \"\"\n",
    "    labels = \"\"\n",
    "\n",
    "    words =  line.strip().split(\" \")\n",
    "    for word in words:\n",
    "        chars += word\n",
    "        \n",
    "        word_len = len(word)\n",
    "        if (word_len == 1):\n",
    "            labels += \"S\"\n",
    "        elif (word_len > 1):\n",
    "            labels += \"B\"+\"I\"*(word_len-2)+\"E\"\n",
    "\n",
    "    return chars, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset(\"msr_pku_training.utf8\", \"msr_pku_training_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_path: str, label_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    :param input_path; Path to the input dataset\n",
    "    :param label_path; Path to the file containing the corresponding labels for the input dataset\n",
    "    :return sentences; List of sentences in input_file\n",
    "    :return labels; List of corresponding word segment codes in label_path. Same len as sentences\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        for line in file:\n",
    "            sentences.append(line.strip())\n",
    "\n",
    "    labels = []\n",
    "    with open(label_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        for line in file:\n",
    "            labels.append(line.strip())\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "def get_unigrams(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param sentence; A line from the dataset as str\n",
    "    :return unigrams; List of unigrams in the line\n",
    "    :return bigrams; List of bigrams in the line\n",
    "    \"\"\"\n",
    "    unigrams = []\n",
    "\n",
    "    sentence_len = len(sentence)\n",
    "\n",
    "    for k in range(sentence_len-1):\n",
    "        unigrams.append(sentence[k])\n",
    "\n",
    "    unigrams.append(sentence[sentence_len-1])\n",
    "\n",
    "    return unigrams\n",
    "\n",
    "def make_X_vocab(sentences: List[str]) -> Dict[str, int]:\n",
    "    '''\n",
    "    :param sentences; List of input sentences from the dataset\n",
    "    :return unigrams_vocab; Dictionary from unigram to int\n",
    "    :return bigrams_vocab; Dictionary from bigram to int\n",
    "    '''\n",
    "    vocab = {\"UNK\": 0}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        unigrams = get_unigrams(sentence)\n",
    "\n",
    "        for k in range(len(unigrams)):\n",
    "            if unigrams[k] not in vocab:\n",
    "                vocab[unigrams[k]] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def make_Y_vocab(labels: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    :param labels; List of label codes\n",
    "    :return labels_vocab; Dictionary from label code to int \n",
    "    \"\"\"\n",
    "    #labels_vocab = {\"UNK\": 0}\n",
    "    labels_vocab = dict()\n",
    "    for label_line in labels:\n",
    "        for label in label_line:\n",
    "            if label not in labels_vocab:\n",
    "                labels_vocab[label] = len(labels_vocab)\n",
    "\n",
    "    return labels_vocab\n",
    "\n",
    "def make_X(sentences: List[str], vocab: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param sentences; List of sentences\n",
    "    :param unigrams_vocab; Unigram vocabulary\n",
    "    :param bigrams_vocab; Bigram vocabulary\n",
    "    :return X; Matrix storing all sentences' feature vector \n",
    "    \"\"\"\n",
    "    X1 = []\n",
    "    for sentence in sentences:\n",
    "        x_temp = []\n",
    "        unigrams = get_unigrams(sentence)\n",
    "        for i in range(len(unigrams)):\n",
    "            x_temp.append(vocab[unigrams[i]])\n",
    "\n",
    "        X1.append(np.array(x_temp))\n",
    "\n",
    "    X1 = np.array(X1)\n",
    "    return X1\n",
    "\n",
    "def make_Y(labels: List[str], labels_vocab: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param labels; List of word segment codes, line by line\n",
    "    :param labels_vocab; Label codes vocab\n",
    "    :return y; Vector of label code indices\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    one_hot = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "    for label_line in labels:\n",
    "        y_temp = []\n",
    "        for label in label_line:\n",
    "            y_temp.append( one_hot [labels_vocab[label]] )\n",
    "            #y_temp.append( labels_vocab[label] )\n",
    "        y.append(np.array(y_temp))\n",
    "    \n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = load_dataset(\"msr_pku_training__input.utf8\", \"msr_pku_training__labels.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105972\n",
      "48\n",
      "“人们常说生活是一部教科书，而血与火的战争更是不可多得的教科书，她确实是名副其实的‘我的大学’。\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(sentences[0]))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105972\n",
      "48\n",
      "SBESSBESSSBIESSSSSSBESSBIIESBIESSBESBIIESSSSBESS\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(labels[0]))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5367\n"
     ]
    }
   ],
   "source": [
    "vocab = make_X_vocab(sentences)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 0, 'B': 1, 'E': 2, 'I': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_vocab = make_Y_vocab(labels)\n",
    "labels_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_X(sentences, vocab)\n",
    "y = make_Y(labels, labels_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = pad_sequences(X, truncating='pre', padding='post', maxlen=50)\n",
    "y_ = pad_sequences(y, truncating='pre', padding='post', maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105972, 50)\n",
      "(105972, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x2, dev_x2, train_y2, dev_y2 = train_test_split(X_, y_, test_size=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100673, 50)\n",
      "(100673, 50, 4)\n",
      "(5299, 50)\n",
      "(5299, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_x2.shape)\n",
    "print(train_y2.shape)\n",
    "print(dev_x2.shape)\n",
    "print(dev_y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the vocabularies to file\n",
    "\n",
    "with open('x_vocab_uni_only.utf8', 'w', encoding=\"utf-8-sig\") as file:\n",
    "    file.write(json.dumps(vocab))\n",
    "    \n",
    "with open('y_vocab.utf8', 'w', encoding=\"utf-8-sig\") as file:\n",
    "    file.write(json.dumps(labels_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class helps with logging\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        if context.executing_eagerly():\n",
    "            self.val_writer = tf.contrib.summary.create_file_writer(self.val_log_dir)\n",
    "        else:\n",
    "            self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def _write_custom_summaries(self, step, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if 'val_' in k}\n",
    "        if context.executing_eagerly():\n",
    "            with self.val_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "                for name, value in val_logs.items():\n",
    "                    tf.contrib.summary.scalar(name, value.item(), step=step)\n",
    "        else:\n",
    "            for name, value in val_logs.items():\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.val_writer.add_summary(summary, step)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        logs = {k: v for k, v in logs.items() if not 'val_' in k}\n",
    "        super(TrainValTensorBoard, self)._write_custom_summaries(step, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please take note that most of this part was extracted from class exercises, with some additions\n",
    "\n",
    "def create_keras_model(vocab_size, embedding_size=64, hidden_size=256):\n",
    "    print(\"Creating KERAS model\")\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, mask_zero=True))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer = K.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a pre-saved model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          343488    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 512)         657408    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 4)           2052      \n",
      "=================================================================\n",
      "Total params: 4,152,772\n",
      "Trainable params: 4,152,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "model_name = \"uni_only_50pad_3bilstm.hdf5\"\n",
    "\n",
    "#checks if the FINAL model was saved and loads it instead of creating a new one\n",
    "if os.path.exists(model_name):\n",
    "    model = load_model(model_name)\n",
    "    print(\"Using a pre-saved model\")\n",
    "    model.summary()\n",
    "else:\n",
    "    model = create_keras_model(vocab_size)\n",
    "    model.summary()\n",
    "    print(\"Training a new model\")\n",
    "    \n",
    "    filepath = \"models/uni_only_50pad_3bilstm-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = K.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "    callbacks_list = [TrainValTensorBoard(write_graph=False), checkpoint]\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    model.fit(train_x2, train_y2, epochs=epochs, batch_size=batch_size,\n",
    "              shuffle=True, validation_data=(dev_x2, dev_y2), callbacks=callbacks_list) \n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    #Save the FINAL model for later reuse\n",
    "    model.save(model_name)\n",
    "    print(\"Trained model saved for later use\")\n",
    "\n",
    "    print(\"\\nEvaluating test...\")\n",
    "    loss_acc = model.evaluate(dev_x2, dev_y2, verbose=0)\n",
    "    print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5299, 50, 4)\n",
      "(5299, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "pred_dev_y2 = model.predict(dev_x2)\n",
    "print(pred_dev_y2.shape)\n",
    "print(dev_y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9977511e-01 2.6404706e-07 2.2462867e-04 4.9865708e-08]\n",
      " [9.9999952e-01 1.9819115e-09 4.3202360e-07 8.9653760e-08]\n",
      " [9.9096340e-01 9.0339603e-03 2.0147495e-06 7.5938999e-07]\n",
      " [7.4362538e-06 9.9911708e-01 2.3175540e-05 8.5239398e-04]\n",
      " [2.3481037e-09 1.7470664e-06 1.1806457e-05 9.9998641e-01]]\n",
      "['S', 'S', 'S', 'B', 'I']\n",
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]]\n",
      "['S', 'S', 'S', 'B', 'I']\n"
     ]
    }
   ],
   "source": [
    "#Preview predicted predictions for the first five characters of the first line\n",
    "\n",
    "print(pred_dev_y2[0,0:5])\n",
    "print([id_to_label[k] for k in np.argmax(pred_dev_y2[0,0:5], 1)])\n",
    "\n",
    "print(dev_y2[0,0:5])\n",
    "print([id_to_label[k] for k in np.argmax(dev_y2[0,0:5], 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {v:k for k,v in labels_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = []\n",
    "for label in dev_y2:\n",
    "    val_labels.append([id_to_label[k] for k in np.argmax(label, 1)])\n",
    "    \n",
    "pred_labels = []\n",
    "for pred in pred_dev_y2:\n",
    "    pred_labels.append([id_to_label[k] for k in np.argmax(pred, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
